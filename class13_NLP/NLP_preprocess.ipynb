{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed14597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba8a182",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "[Source Kaggle](https://www.kaggle.com/competitions/sentiment-analysis-on-movie-reviews/data)\n",
    "### classes\n",
    "* 0 - negative\n",
    "* 1 - somewhat negative\n",
    "* 2 - neutral\n",
    "* 3 - somewhat positive\n",
    "* 4 - positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b052a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/EnggQasim/Saylani-AI-Batch3/main/MachineLearning/NLP/train.tsv\", \n",
    "                 sep=\"\\t\")\n",
    "df.info()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f87c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e2d6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.concat([\n",
    "    df[df.Sentiment==0],\n",
    "    df[df.Sentiment==4]\n",
    "])[[\"Phrase\",\"Sentiment\"]]\n",
    "\n",
    "print(len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f7b27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f1dfc1",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a985b414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#pip install autocorrect\n",
    "from autocorrect import Speller\n",
    "import string\n",
    "import timeit\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c086624",
   "metadata": {},
   "source": [
    "### Remove newlines & tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5eac8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is her   first day at this place.  Please,  Be nice to her. '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_newlines_tabs(text):\n",
    "    \"\"\"\n",
    "    This function will remove all the occurrences of newlines, tabs, and combinations like: \\\\n, \\\\.\n",
    "    \n",
    "    arguments:\n",
    "        input_text: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after removal of newlines, tabs, \\\\n, \\\\ characters.\n",
    "        \n",
    "    Example:\n",
    "    Input : This is her \\\\ first day at this place.\\n Please,\\t Be nice to her.\\\\n\n",
    "    Output : This is her first day at this place. Please, Be nice to her. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Replacing all the occurrences of \\n,\\\\n,\\t,\\\\ with a space.\n",
    "    Formatted_text = text.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\\\', ' ').replace('. com', '.com')\n",
    "    return Formatted_text\n",
    "\n",
    "remove_newlines_tabs(\"This is her \\\\ first day at this place.\\n Please,\\t Be nice to her.\\\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bb394d",
   "metadata": {},
   "source": [
    "### Strip Html Tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e712758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This is a  nice  place to live. '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def strip_html_tags(text):\n",
    "    \"\"\" \n",
    "    This function will remove all the occurrences of html tags from the text.\n",
    "    \n",
    "    arguments:\n",
    "        input_text: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after removal of html tags.\n",
    "        \n",
    "    Example:\n",
    "    Input : This is a nice place to live. \n",
    "    Output : This is a nice place to live.  \n",
    "    \"\"\"\n",
    "    # Initiating BeautifulSoup object soup.\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    # Get all the text other than html tags.\n",
    "    stripped_text = soup.get_text(separator=\" \")\n",
    "    return stripped_text\n",
    "\n",
    "\n",
    "strip_html_tags(\" This is a <b>nice</b> place to live. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caab93dc",
   "metadata": {},
   "source": [
    "### Remove Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b289a6a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' website:   visit:  adasfas ad as'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_links(text):\n",
    "    \"\"\"\n",
    "    This function will remove all the occurrences of links.\n",
    "    \n",
    "    arguments:\n",
    "        input_text: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after removal of all types of links.\n",
    "        \n",
    "    Example:\n",
    "    Input : To know more about cats and food & website: catster.com  visit: https://catster.com//how-to-feed-cats\n",
    "\n",
    "    Output : To know more about cats and food & website: visit:     \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Removing all the occurrences of links that starts with https\n",
    "    remove_https = re.sub(r'http\\S+', '', text)\n",
    "    # Remove all the occurrences of text that ends with .com\n",
    "    remove_com = re.sub(r\"\\ [A-Za-z]*\\.com\", \" \", remove_https)\n",
    "    return remove_com\n",
    "\n",
    "remove_links(\" website: catster.com  visit: https://catster.com//how-to-feed-cats adasfas ad as\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e0c28a",
   "metadata": {},
   "source": [
    "# Remove WhiteSpaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bfc4a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How are you doing ?  (pakistan) ABC'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_whitespace(text):\n",
    "    \"\"\" This function will remove \n",
    "        extra whitespaces from the text\n",
    "    arguments:\n",
    "        input_text: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after extra whitespaces removed .\n",
    "        \n",
    "    Example:\n",
    "    Input : How   are   you   doing   ?\n",
    "    Output : How are you doing ?     \n",
    "        \n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'\\s+') \n",
    "    Without_whitespace = re.sub(pattern, ' ', text)\n",
    "    # There are some instances where there is no space after '?' & ')', \n",
    "    # So I am replacing these with one space so that It will not consider two words as one token.\n",
    "    text = Without_whitespace.replace('?', ' ? ').replace(')', ') ')\n",
    "    return text    \n",
    "\n",
    "remove_whitespace(\"How   are  \\t you \\n   doing? (pakistan)ABC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9095e640",
   "metadata": {},
   "source": [
    "### Remove Accented Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75282c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Malaga, aeeohello'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code for accented characters removal\n",
    "def accented_characters_removal(text):\n",
    "    # this is a docstring\n",
    "    \"\"\"\n",
    "    The function will remove accented characters from the \n",
    "    text contained within the Dataset.\n",
    "       \n",
    "    arguments:\n",
    "        input_text: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" with removed accented characters.\n",
    "        \n",
    "    Example:\n",
    "    Input : Málaga, àéêöhello\n",
    "    Output : Malaga, aeeohello    \n",
    "        \n",
    "    \"\"\"\n",
    "    # Remove accented characters from text using unidecode.\n",
    "    # Unidecode() - It takes unicode data & tries to represent it to ASCII characters. \n",
    "    text = unidecode.unidecode(text)\n",
    "    return text\n",
    "\n",
    "accented_characters_removal(\"Málaga, àéêöhello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528244ad",
   "metadata": {},
   "source": [
    "# Case Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e5e6881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pakistan zinda bad!'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code for text lowercasing\n",
    "def lower_casing_text(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    The function will convert text into lower case.\n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "         value: text in lowercase\n",
    "         \n",
    "    Example:\n",
    "    Input : The World is Full of Surprises!\n",
    "    Output : the world is full of surprises!\n",
    "    \n",
    "    \"\"\"\n",
    "    # Convert text to lower case\n",
    "    # lower() - It converts all upperase letter of given string to lowercase.\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "lower_casing_text(\"Pakistan Zinda BAD!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac2a39",
   "metadata": {},
   "source": [
    "### Reduce repeated characters and punctuations¶"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b7654b5",
   "metadata": {},
   "source": [
    "Explanation for using some symbols in above regex expression\n",
    "\\1 —> is equivalent to re.search(...). group(1). It Refers to first capturing group. \\1 matches the exact same text that was matched by the first capturing group.\n",
    "\n",
    "{1,} --> It means we are matching for repeatation that occurs more than one times.\n",
    "\n",
    "DOTALL -> It matches newline character as well unlike dot operator which matches everything in the given text except newline character.\n",
    "\n",
    "sub() --> This function is used to replace occurrences of a particular sub-string with another sub-string. This function takes as input the following: The sub-string to replace. The sub-string to replace with.\n",
    "\n",
    "r'\\1\\1' --> It limits all the repeatation to two characters.\n",
    "\n",
    "r'\\1' --> Limits all the repeatation to only one character.\n",
    "\n",
    "{2,} --> It means to match for repeatation that occurs more than two times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32459968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reallyy, Greeaatt !?.;:)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reducing_incorrect_character_repeatation(text):\n",
    "    \"\"\"\n",
    "    This Function will reduce repeatition to two characters \n",
    "    for alphabets and to one character for punctuations.\n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Finally formatted text with alphabets repeating to \n",
    "        two characters & punctuations limited to one repeatition \n",
    "        \n",
    "    Example:\n",
    "    Input : Realllllllllyyyyy,        Greeeeaaaatttt   !!!!?....;;;;:)\n",
    "    Output : Reallyy, Greeaatt !?.;:)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Pattern matching for all case alphabets\n",
    "    Pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n",
    "    \n",
    "    # Limiting all the  repeatation to two characters.\n",
    "    Formatted_text = Pattern_alpha.sub(r\"\\1\\1\", text) \n",
    "    \n",
    "    # Pattern matching for all the punctuations that can occur\n",
    "    Pattern_Punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n",
    "    \n",
    "    # Limiting punctuations in previously formatted string to only one.\n",
    "    Combined_Formatted = Pattern_Punct.sub(r'\\1', Formatted_text)\n",
    "    \n",
    "    # The below statement is replacing repeatation of spaces that occur more than two times with that of one occurrence.\n",
    "    Final_Formatted = re.sub(' {2,}',' ', Combined_Formatted)\n",
    "    return Final_Formatted\n",
    "\n",
    "reducing_incorrect_character_repeatation(\"Realllllllllyyyyy,        Greeeeaaaatttt   !!!!?....;;;;:)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a1a6a0",
   "metadata": {},
   "source": [
    "# Expand contraction words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bb6c000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"is not , are not , cannot , cause , cannot've\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\",\n",
    "}\n",
    "# The code for expanding contraction words\n",
    "def expand_contractions(text, contraction_mapping =  CONTRACTION_MAP):\n",
    "    \"\"\"expand shortened words to the actual form.\n",
    "       e.g. don't to do not\n",
    "    \n",
    "       arguments:\n",
    "            input_text: \"text\" of type \"String\".\n",
    "         \n",
    "       return:\n",
    "            value: Text with expanded form of shorthened words.\n",
    "        \n",
    "       Example: \n",
    "       Input : ain't, aren't, can't, cause, can't've\n",
    "       Output :  is not, are not, cannot, because, cannot have \n",
    "    \n",
    "     \"\"\"\n",
    "    # Tokenizing text into tokens.\n",
    "    list_Of_tokens = text.split(' ')\n",
    "\n",
    "    # Checking for whether the given token matches with the Key & replacing word with key's value.\n",
    "    \n",
    "    # Check whether Word is in lidt_Of_tokens or not.\n",
    "    for Word in list_Of_tokens: \n",
    "        # Check whether found word is in dictionary \"Contraction Map\" or not as a key. \n",
    "         if Word in CONTRACTION_MAP: \n",
    "                # If Word is present in both dictionary & list_Of_tokens, replace that word with the key value.\n",
    "                list_Of_tokens = [item.replace(Word, CONTRACTION_MAP[Word]) for item in list_Of_tokens]\n",
    "                \n",
    "    # Converting list of tokens to String.\n",
    "    String_Of_tokens = ' '.join(str(e) for e in list_Of_tokens) \n",
    "    return String_Of_tokens    \n",
    "\n",
    "expand_contractions(\"ain't , aren't , can't , cause , can't've\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94254bc",
   "metadata": {},
   "source": [
    "# Remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0e05cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hello, K a j a l. Thi*s is $100.05 : the payment that you will recieve! (Is this okay?) '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The code for removing special characters\n",
    "def removing_special_characters(text):\n",
    "    \"\"\"Removing all the special characters except the one that is passed within \n",
    "       the regex to match, as they have imp meaning in the text provided.\n",
    "   \n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text with removed special characters that don't require.\n",
    "        \n",
    "    Example: \n",
    "    Input : Hello, K-a-j-a-l. Thi*s is $100.05 : the payment that you will recieve! (Is this okay?) \n",
    "    Output :  Hello, Kajal. This is $100.05 : the payment that you will recieve! Is this okay?\n",
    "    \n",
    "   \"\"\"\n",
    "    # The formatted text after removing not necessary punctuations.\n",
    "    Formatted_Text = re.sub(r\"[^a-zA-Z0-9:$-,%.?!]+\", ' ', text) \n",
    "    # In the above regex expression,I am providing necessary set of punctuations that are frequent in this particular dataset.\n",
    "    return Formatted_Text\n",
    "\n",
    "removing_special_characters(\" Hello, K-a-j-a-l. Thi*s is $100.05 : the payment that you will recieve! (Is this okay?) \")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab2616fe",
   "metadata": {},
   "source": [
    "Punctuations that I am considering Important as per my Dataset.\n",
    ",.?! --> These are some frequent punctuations that occurs a lot and needed to preserved as to understand the context of text.\n",
    "\n",
    ": --> This one is also frequent as per the Dataset. It is important to keep bcz it is giving sense whenever there is a occurrence of time like: 9:05 p.m.\n",
    "\n",
    "% --> This one is also frequently used in many articles and telling more precisely about the data, facts & figures.\n",
    "\n",
    "$ --> This one is used in many articles where prices are considered. So, omitting this symbol will not give much sense about those prices that left as just some numbers only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bde201d",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a5aa6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dadbe176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'This Kajal delhi came study . '\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The code for removing stopwords\n",
    "stoplist = stopwords.words('english') \n",
    "\n",
    "stoplist = set(stoplist)\n",
    "def removing_stopwords(text):\n",
    "    \"\"\"This function will remove stopwords which doesn't add much meaning to a sentence \n",
    "       & they can be remove safely without comprimising meaning of the sentence.\n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text after omitted all stopwords.\n",
    "        \n",
    "    Example: \n",
    "    Input : This is Kajal from delhi who came here to study.\n",
    "    Output : [\"'This\", 'Kajal', 'delhi', 'came', 'study', '.', \"'\"] \n",
    "    \n",
    "   \"\"\"\n",
    "    # repr() function actually gives the precise information about the string\n",
    "    text = repr(text)\n",
    "    # Text without stopwords\n",
    "    No_StopWords = [word for word in word_tokenize(text) if word.lower() not in stoplist ]\n",
    "    # Convert list of tokens_without_stopwords to String type.\n",
    "    words_string = ' '.join(No_StopWords)    \n",
    "    return words_string\n",
    "\n",
    "removing_stopwords(\"This is Kajal from the delhi who was came here to study.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af91ee",
   "metadata": {},
   "source": [
    "# Correct mis-spelled words in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c50bfe6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is Oberoi from Delhi who came here to study.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The code for spelling corrections\n",
    "def spelling_correction(text):\n",
    "    ''' \n",
    "    This function will correct spellings.\n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text after corrected spellings.\n",
    "        \n",
    "    Example: \n",
    "    Input : This is Oberois from Dlhi who came heree to studdy.\n",
    "    Output : This is Oberoi from Delhi who came here to study.\n",
    "      \n",
    "    \n",
    "    '''\n",
    "    # Check for spellings in English language\n",
    "    spell = Speller(lang='en')\n",
    "    Corrected_text = spell(text)\n",
    "    return Corrected_text\n",
    "\n",
    "spelling_correction(\"This is Oberois from Dlhi who came heree to studdy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d34703f",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f4e5d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfe8b328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ext',\n",
       " 'have',\n",
       " 'root',\n",
       " 'word',\n",
       " 'only,',\n",
       " 'no',\n",
       " 'tense',\n",
       " 'form,',\n",
       " 'no',\n",
       " 'plural',\n",
       " 'form',\n",
       " 'cat',\n",
       " 'dog']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The code for lemmatization\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatization(text):\n",
    "    \"\"\"This function converts word to their root words \n",
    "       without explicitely cut down as done in stemming.\n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text having root words only, no tense form, no plural forms\n",
    "        \n",
    "    Example: \n",
    "    Input : text reduced \n",
    "    Output :  text reduce\n",
    "    \n",
    "   \"\"\"\n",
    "    # Converting words to their root forms\n",
    "    lemma = [lemmatizer.lemmatize(w,'v') for w in w_tokenizer.tokenize(text)]\n",
    "    return lemma\n",
    "\n",
    "lemmatization(\"ext having root words only, no tense form, no plural forms cats dogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cab126a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'educations'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('educations','v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7a27ca",
   "metadata": {},
   "source": [
    "# Putting all in single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "464436cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\", 'would', 'hard', 'time', 'sit', 'one', 'dog', \"'\"]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Writing main function to merge all the preprocessing steps.\n",
    "def text_preprocessing(text, accented_chars=True, contractions=True, lemmatization_allow = True,\n",
    "                        extra_whitespace=True, newlines_tabs=True, repeatition=True, \n",
    "                       lowercase=True, punctuations=True, mis_spell=True,\n",
    "                       remove_html=True, links=True,  special_chars=True,\n",
    "                       stop_words=True):\n",
    "    \"\"\"\n",
    "    This function will preprocess input text and return\n",
    "    the clean text.\n",
    "    \"\"\"\n",
    "    \n",
    "    if newlines_tabs == True: #remove newlines & tabs.\n",
    "        Data = remove_newlines_tabs(text)\n",
    "        \n",
    "    if remove_html == True: #remove html tags\n",
    "        Data = strip_html_tags(Data)\n",
    "        \n",
    "    if links == True: #remove links\n",
    "        Data = remove_links(Data)\n",
    "        \n",
    "    if extra_whitespace == True: #remove extra whitespaces\n",
    "        Data = remove_whitespace(Data)\n",
    "        \n",
    "    if accented_chars == True: #remove accented characters\n",
    "        Data = accented_characters_removal(Data)\n",
    "        \n",
    "    if lowercase == True: #convert all characters to lowercase\n",
    "        Data = lower_casing_text(Data)\n",
    "        \n",
    "    if repeatition == True: #Reduce repeatitions   \n",
    "        Data = reducing_incorrect_character_repeatation(Data)\n",
    "        \n",
    "    if contractions == True: #expand contractions\n",
    "        Data = expand_contractions(Data)\n",
    "    \n",
    "    if punctuations == True: #remove punctuations\n",
    "        Data = removing_special_characters(Data)\n",
    "    \n",
    "    stoplist = stopwords.words('english') \n",
    "    stoplist = set(stoplist)\n",
    "    \n",
    "    if stop_words == True: #Remove stopwords\n",
    "        Data = removing_stopwords(Data)\n",
    "        \n",
    "    spell = Speller(lang='en')\n",
    "    \n",
    "    if mis_spell == True: #Check for mis-spelled words & correct them.\n",
    "        Data = spelling_correction(Data)\n",
    "        \n",
    "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "#     print(Data)\n",
    "    if lemmatization_allow == True: #Converts words to lemma form.\n",
    "        Data = lemmatization(Data)\n",
    "    \n",
    "           \n",
    "    return Data\n",
    "text_preprocessing(' would have a hard time sitting through this one dogs ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7051ff88",
   "metadata": {},
   "source": [
    "# Now apply function on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce8e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f325c55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df1['input_data'] = df1.Phrase.apply(text_preprocessing)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf1e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.Phrase = df1.Phrase.apply(str)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974911ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_pickle(\"data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9ab35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_pickle(\"data.pkl\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5040555",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"\"\"\n",
    "Rizwan Ali4:34 PM\n",
    "Sir i couldn't attend todays class physically because i am suffering from fiver.\n",
    "Farrukh Jalil4:52 PM\n",
    "4210113978763\n",
    "Rizwan Ali4:53 PM\n",
    "4230174256393\n",
    "Syed Abbas Abbas4:53 PM\n",
    "1234567890123\n",
    "syed daniyal4:53 PM\n",
    "4250189782769\n",
    "Agha Zohaib4:53 PM\n",
    "4210168710111\n",
    "Syed Zaim Nazir4:53 PM\n",
    "4220171026219\n",
    "Kenneth Fahad4:54 PM\n",
    "3310541597331\n",
    "Syed Sajjad Ali Shah4:54 PM\n",
    "1510120026221\n",
    "Javeria Abbasi4:55 PM\n",
    "4220140843098\n",
    "Muneeb Abdul Rauf4:56 PM\n",
    "4230181544787\n",
    "TARIQ JAMIL5:00 PM\n",
    "42501-8229854-5\n",
    "Syed Mohammad Asad Asad5:03 PM\n",
    "4240180561365\n",
    "Syeda Zehra Nadeem5:03 PM\n",
    "4230123587896\n",
    "Muhammad Fahad5:03 PM\n",
    "4210117498405\n",
    "Nadir Ali5:10 PM\n",
    "4130311530273\n",
    "Syed Zaim Nazir5:54 PM\n",
    "fibonacci\n",
    "Ahmed Saud5:57 PM\n",
    "4210176752885\n",
    "Bilal Uddin5:59 PM\n",
    "4220122191357\n",
    "Imran Ali6:00 PM\n",
    "4210176950677\n",
    "Zoya Ansari6:05 PM\n",
    "4220186465696\n",
    "Amna Shahid6:05 PM\n",
    "3460348242756\n",
    "You6:06 PM\n",
    "https://github.com/EnggQasim/Saylani-AI-Batch3/blob/main/MachineLearning/NLP/NLP_preprocess.ipynb\n",
    "mcc-qjtf-rix\n",
    "\"\"\".replace(\"-\",\"\")\n",
    "\n",
    "data = pd.DataFrame(list(set(re.findall(\"\\d{13}\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571282a8",
   "metadata": {},
   "source": [
    "# https://www.analyticsvidhya.com/blog/2021/06/part-5-step-by-step-guide-to-master-nlp-text-vectorization-approaches/#:~:text=To%20convert%20the%20text%20data,text%20data%20to%20numerical%20vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf6a60",
   "metadata": {},
   "source": [
    "# https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0013ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_,\"Vocab\")\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape,\"Shape\")\n",
    "print(type(vector),\"Type\")\n",
    "print(vector.toarray(),\"Array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23308396",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = [\"the fox jumped\"]\n",
    "vectorizer.transform(text1).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaad809",
   "metadata": {},
   "source": [
    " ## TF-IDF \n",
    "* Term Frequency: This summarizes how often a given word appears within a document.\n",
    "* Inverse Document Frequency: This downscales words that appear a lot across documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cfa374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    " \"The dog.\",\n",
    " \"The fox\"]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n",
    "# encode document\n",
    "vector = vectorizer.transform([text[0]])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b471ceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "# n-gram\n",
    "# TF-IDF\n",
    "# Hashing with HashingVectorizer\n",
    "\n",
    "\n",
    "# WordEmbeding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
